{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4891afd0-61fa-470f-ba43-4fc92cca9e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q sacrebleu rouge-score bert-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33139841-8549-41e1-b7e0-875e2d182664",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [13:04<00:00,  1.27it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2df736212a6c4af8a46302952edf177b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b8eca92bb58409dbccfc4e20f692379",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "230dc04858a4456fa46d02e9802dc3d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "143ae7dfb4164e509cae801567bf7e4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a50533b60cf24299a1dabbdd155a899d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f241a866d41c4d57978c637cab35e743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation Metrics ===\n",
      "Exact Match: 0.0000\n",
      "BLEU Score: 5.17\n",
      "ROUGE-L F1: 0.2332\n",
      "BERTScore F1: 0.8346\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from rouge_score import rouge_scorer\n",
    "from sacrebleu.metrics import BLEU\n",
    "import bert_score\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the fine-tuned model\n",
    "def load_model(model_path):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(device)\n",
    "    model.eval()\n",
    "    return tokenizer, model\n",
    "\n",
    "# SQL generation function\n",
    "def generate_sql(model, tokenizer, sql_prompt, sql_context, max_length=128):\n",
    "    input_text = f\"sql_prompt: {sql_prompt} sql_context: {sql_context}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        output_tokens = model.generate(**inputs, max_new_tokens=max_length, min_new_tokens=5)\n",
    "    return tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer, model = load_model(\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Load test dataset\n",
    "dataset = load_dataset(\"gretelai/synthetic_text_to_sql\")\n",
    "test_data = dataset[\"test\"].select(range(1000))\n",
    "\n",
    "# Compute metrics\n",
    "exact_matches = []\n",
    "references = []\n",
    "predictions = []\n",
    "\n",
    "for example in tqdm(test_data):\n",
    "    prompt = example[\"sql_prompt\"]\n",
    "    context = example[\"sql_context\"]\n",
    "    ground_truth = example[\"sql\"]\n",
    "\n",
    "    prediction = generate_sql(model, tokenizer, prompt, context)\n",
    "\n",
    "    references.append(ground_truth)\n",
    "    predictions.append(prediction)\n",
    "    exact_matches.append(int(prediction.strip().lower() == ground_truth.strip().lower()))\n",
    "\n",
    "# Exact Match Accuracy\n",
    "em_score = sum(exact_matches) / len(exact_matches)\n",
    "\n",
    "# BLEU Score\n",
    "bleu = BLEU()\n",
    "bleu_score = bleu.corpus_score(predictions, [references]).score\n",
    "\n",
    "# ROUGE-L\n",
    "scorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
    "rouge_l_scores = [scorer.score(ref, pred)[\"rougeL\"].fmeasure for ref, pred in zip(references, predictions)]\n",
    "avg_rouge_l = sum(rouge_l_scores) / len(rouge_l_scores)\n",
    "\n",
    "# BERTScore\n",
    "P, R, F1 = bert_score.score(predictions, references, lang=\"en\", device=device)\n",
    "bert_f1 = F1.mean()\n",
    "\n",
    "# Print metrics\n",
    "print(\"\\n=== Evaluation Metrics ===\")\n",
    "print(f\"Exact Match: {em_score:.4f}\")\n",
    "print(f\"BLEU Score: {bleu_score:.2f}\")\n",
    "print(f\"ROUGE-L F1: {avg_rouge_l:.4f}\")\n",
    "print(f\"BERTScore F1: {bert_f1.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10a5e466-e981-4401-905f-aa3b7a1701cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1243bf08-430d-41eb-bf56-8ddb1f65056a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [08:01<00:00,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Execution Accuracy: 55.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import sqlite3\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "def load_model(model_path):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "    model.eval()\n",
    "    return tokenizer, model\n",
    "\n",
    "def generate_sql(model, tokenizer, sql_prompt, sql_context, max_length=128):\n",
    "    input_text = f\"sql_prompt: {sql_prompt} sql_context: {sql_context}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=512).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        output_tokens = model.generate(**inputs, max_new_tokens=max_length, min_new_tokens=5)\n",
    "    return tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Execute SQL on SQLite DB\n",
    "# -------------------------------\n",
    "def run_query(context, query):\n",
    "    try:\n",
    "        conn = sqlite3.connect(\":memory:\")\n",
    "        cur = conn.cursor()\n",
    "        # Split context into separate statements and execute\n",
    "        for stmt in context.strip().split(\";\"):\n",
    "            stmt = stmt.strip()\n",
    "            if stmt:\n",
    "                cur.execute(stmt + \";\")\n",
    "        # Run the predicted/ground truth query\n",
    "        cur.execute(query)\n",
    "        results = cur.fetchall()\n",
    "        conn.close()\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "def compute_execution_accuracy(dataset, model, tokenizer):\n",
    "    correct = 0\n",
    "    total = len(dataset)\n",
    "\n",
    "    for sample in tqdm(dataset):\n",
    "        sql_prompt = sample[\"sql_prompt\"]\n",
    "        sql_context = sample[\"sql_context\"]\n",
    "        gt_sql = sample[\"sql\"]\n",
    "\n",
    "        pred_sql = generate_sql(model, tokenizer, sql_prompt, sql_context)\n",
    "\n",
    "        result_gt = run_query(sql_context, gt_sql)\n",
    "        result_pred = run_query(sql_context, pred_sql)\n",
    "\n",
    "        if result_gt == result_pred:\n",
    "            correct += 1\n",
    "        else:\n",
    "            # print(\"\\n---\")\n",
    "            # print(\"Prompt:\", sql_prompt)\n",
    "            # print(\"Predicted:\", pred_sql)\n",
    "            # print(\"Expected:\", gt_sql)\n",
    "            # print(\"GT Result:\", result_gt)\n",
    "            # print(\"Pred Result:\", result_pred)\n",
    "            ff = 0\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "\n",
    "tokenizer, model = load_model(\"nl2sql_epoch2\") \n",
    "model = model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "dataset = load_dataset(\"gretelai/synthetic_text_to_sql\")[\"test\"]\n",
    "subset = [dataset[i] for i in range(1000)]\n",
    "exec_acc = compute_execution_accuracy(subset, model, tokenizer)\n",
    "print(f\"\\nExecution Accuracy: {exec_acc:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81159fe-af6c-4c4d-9f62-e1bd99e83e4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
