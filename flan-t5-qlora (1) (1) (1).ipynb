{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "!pip install -q pandas numpy torch datasets transformers peft bitsandbytes evaluate rapidfuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from datasets import Dataset, DatasetDict, load_from_disk\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback  \n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import BitsAndBytesConfig \n",
    "\n",
    "import evaluate\n",
    "from rapidfuzz import fuzz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U transformers bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm  # For progress bar\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "from datasets import Dataset, DatasetDict, load_from_disk\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, TrainingArguments, Trainer\n",
    "from transformers.trainer_callback import EarlyStoppingCallback\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback,\n",
    "    BitsAndBytesConfig  \n",
    ")\n",
    "import evaluate\n",
    "from rapidfuzz import fuzz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading GretelAI dataset...\n",
      "Rows before dropping duplicates: 100000\n",
      "Rows after dropping duplicates: 100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (597 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after token length filtering: 99906\n",
      "Data splits - Train: 84920 Test: 9990 Validation: 4996\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c3531595d3848ae8f26c871efc2e3c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/84920 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "851eaa2d64ad4699855d2ae00fda8de8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/9990 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63b1f79048b1479485d77eb00bd610e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/4996 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved GretelAI dataset successfully!\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Enable cudnn benchmark for fixed input sizes (can speed up computation)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "def clear_memory():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def preprocess(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    return re.sub(r'\\s+', ' ', text.replace('\\n', ' ')).strip()\n",
    "\n",
    "\n",
    "def clean_df(df, rename=None, drop=None, select=None):\n",
    "    if drop:\n",
    "        df = df.drop(columns=drop, errors='ignore')\n",
    "    if rename:\n",
    "        df = df.rename(columns=rename)\n",
    "    for col in ['query', 'context', 'response']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(preprocess)\n",
    "    if select:\n",
    "        df = df[select]\n",
    "    return df\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "max_length_prompt = 500\n",
    "max_length_response = 250\n",
    "\n",
    "def tokenize_length_filter(row):\n",
    "    start_prompt = \"Context:\\n\"\n",
    "    middle_prompt = \"\\n\\nQuery:\\n\"\n",
    "    end_prompt = \"\\n\\nResponse:\\n\"\n",
    "    prompt = f\"{start_prompt}{row['context']}{middle_prompt}{row['query']}{end_prompt}\"\n",
    "    prompt_tokens = tokenizer.encode(prompt, add_special_tokens=True, truncation=False)\n",
    "    response_tokens = tokenizer.encode(row['response'], add_special_tokens=True, truncation=False)\n",
    "    return len(prompt_tokens) <= max_length_prompt and len(response_tokens) <= max_length_response\n",
    "\n",
    "def split_dataframe(df, train_frac=0.85, test_frac=0.1, val_frac=0.05):\n",
    "    n = len(df)\n",
    "    train_end = int(n * train_frac)\n",
    "    test_end = train_end + int(n * test_frac)\n",
    "    train_df = df.iloc[:train_end].reset_index(drop=True)\n",
    "    test_df = df.iloc[train_end:test_end].reset_index(drop=True)\n",
    "    val_df = df.iloc[test_end:].reset_index(drop=True)\n",
    "    return train_df, test_df, val_df\n",
    "\n",
    "# --- DATA LOADING AND PROCESSING ---\n",
    "\n",
    "print(\"Loading GretelAI dataset...\")\n",
    "df3 = pd.read_parquet(\"hf://datasets/gretelai/synthetic_text_to_sql/synthetic_text_to_sql_train.snappy.parquet\")\n",
    "\n",
    "df3 = clean_df(\n",
    "    df3,\n",
    "    rename={'sql_prompt': 'query', 'sql_context': 'context', 'sql': 'response'},\n",
    "    select=['query', 'context', 'response']\n",
    ")\n",
    "\n",
    "print(\"Rows before dropping duplicates:\", len(df3))\n",
    "df3 = df3.dropna(subset=['query', 'context', 'response']).drop_duplicates()\n",
    "print(\"Rows after dropping duplicates:\", len(df3))\n",
    "\n",
    "df3 = df3[df3.apply(tokenize_length_filter, axis=1)]\n",
    "print(\"Rows after token length filtering:\", len(df3))\n",
    "\n",
    "# Split into train/test/val\n",
    "train_df, test_df, val_df = split_dataframe(df3)\n",
    "print(\"Data splits - Train:\", len(train_df), \"Test:\", len(test_df), \"Validation:\", len(val_df))\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"test\": test_dataset,\n",
    "    \"validation\": val_dataset\n",
    "})\n",
    "\n",
    "dataset.save_to_disk(\"gretelai_dataset\")\n",
    "print(\"Saved GretelAI dataset successfully!\")\n",
    "clear_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example from test set: {'query': 'What is the average CO2 emission for each garment manufacturing process?', 'context': 'CREATE TABLE emissions (emission_id INT, garment_type VARCHAR(50), manufacturing_process VARCHAR(50), co2_emission DECIMAL(10, 2));', 'response': 'SELECT garment_type, AVG(co2_emission) FROM emissions GROUP BY garment_type;'}\n",
      "Tokenized dataset not found. Creating a new one...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fec55d9250c1409688f325a232a85b34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/84920 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0b44f02b1144de98a7db9c5bf13fc95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/9990 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96f28452026245cc9f69075081769c4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4996 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "589639f0de674b5da7804ddec0edc032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/84920 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5fef2fff5e8496d92c00f3e6d37cb6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/9990 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24323d45afd34a55902f38518880e6be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/4996 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized dataset saved.\n",
      "Tokenized dataset splits: dict_keys(['train', 'test', 'validation'])\n"
     ]
    }
   ],
   "source": [
    "# Reload dataset \n",
    "dataset = load_from_disk(\"gretelai_dataset\")\n",
    "print(\"Example from test set:\", dataset[\"test\"][0])\n",
    "\n",
    "# Function to tokenize a batch of examples (creates prompt from context and query)\n",
    "def tokenize_function(batch: dict) -> dict:\n",
    "    start_prompt = \"Context:\\n\"\n",
    "    middle_prompt = \"\\n\\nQuery:\\n\"\n",
    "    end_prompt = \"\\n\\nResponse:\\n\"\n",
    "    prompts = [f\"{start_prompt}{ctx}{middle_prompt}{qry}{end_prompt}\"\n",
    "               for ctx, qry in zip(batch[\"context\"], batch[\"query\"])]\n",
    "    tokenized_inputs = tokenizer(prompts, padding=\"max_length\", truncation=True, max_length=512)\n",
    "    tokenized_labels = tokenizer(batch[\"response\"], padding=\"max_length\", truncation=True, max_length=256)\n",
    "    labels = [[-100 if token == tokenizer.pad_token_id else token for token in seq]\n",
    "              for seq in tokenized_labels[\"input_ids\"]]\n",
    "    batch[\"input_ids\"] = tokenized_inputs[\"input_ids\"]\n",
    "    batch[\"attention_mask\"] = tokenized_inputs[\"attention_mask\"]\n",
    "    batch[\"labels\"] = labels\n",
    "    return batch\n",
    "\n",
    "# Try to load the tokenized dataset\n",
    "try:\n",
    "    tokenized_datasets = load_from_disk(\"tokenized_datasets\")\n",
    "    print(\"Loaded tokenized dataset from disk.\")\n",
    "except Exception as e:\n",
    "    print(\"Tokenized dataset not found. Creating a new one...\")\n",
    "    tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"query\", \"context\", \"response\"], num_proc=4)\n",
    "    tokenized_datasets.save_to_disk(\"tokenized_datasets\")\n",
    "    print(\"Tokenized dataset saved.\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "print(\"Tokenized dataset splits:\", tokenized_datasets.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hf_xet\n",
      "  Downloading hf_xet-1.0.3-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (494 bytes)\n",
      "Downloading hf_xet-1.0.3-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 MB\u001b[0m \u001b[31m164.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: hf_xet\n",
      "Successfully installed hf_xet-1.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install hf_xet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      " Context:\n",
      "CREATE TABLE emissions (emission_id INT, garment_type VARCHAR(50), manufacturing_process VARCHAR(50), co2_emission DECIMAL(10, 2));\n",
      "\n",
      "Query:\n",
      "What is the average CO2 emission for each garment manufacturing process?\n",
      "\n",
      "Response:\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "HUMAN RESPONSE:\n",
      " SELECT garment_type, AVG(co2_emission) FROM emissions GROUP BY garment_type;\n",
      "----------------------------------------------------------------------------------------------------\n",
      "BASELINE MODEL OUTPUT (ZERO SHOT):\n",
      " 10 - 2\n",
      "Fine-tuned model not found. Initializing model for QLORA fine-tuning...\n",
      "Base model loaded and prepared for QLORA fine-tuning.\n",
      "Finetuned model is on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# --- MODEL INITIALIZATION ---\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, BitsAndBytesConfig\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "\n",
    "model_name = \"google/flan-t5-base\"\n",
    "\n",
    "# Load the baseline (original) model for inference/testing\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "original_model = original_model.to(device)\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Test a sample prompt from the test set\n",
    "index = 0\n",
    "query = dataset[\"test\"][index][\"query\"]\n",
    "context = dataset[\"test\"][index][\"context\"]\n",
    "response = dataset[\"test\"][index][\"response\"]\n",
    "prompt = f\"Context:\\n{context}\\n\\nQuery:\\n{query}\\n\\nResponse:\\n\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "baseline_output = tokenizer.decode(\n",
    "    original_model.generate(inputs[\"input_ids\"], max_new_tokens=200)[0],\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(\"-\" * 100)\n",
    "print(\"INPUT PROMPT:\\n\", prompt)\n",
    "print(\"-\" * 100)\n",
    "print(\"HUMAN RESPONSE:\\n\", response)\n",
    "print(\"-\" * 100)\n",
    "print(\"BASELINE MODEL OUTPUT (ZERO SHOT):\\n\", baseline_output)\n",
    "\n",
    "clear_memory()\n",
    "\n",
    "# --- LOAD FINETUNED MODEL OR INITIALIZE FOR QLORA TRAINING ---\n",
    "\n",
    "to_train = True\n",
    "try:\n",
    "    finetuned_model = AutoModelForSeq2SeqLM.from_pretrained(\"text2sql_flant5base_finetuned\")\n",
    "    finetuned_model = finetuned_model.to(device)\n",
    "    print(\"Fine-tuned model loaded successfully.\")\n",
    "    to_train = False\n",
    "except Exception as e:\n",
    "    print(\"Fine-tuned model not found. Initializing model for QLORA fine-tuning...\")\n",
    "\n",
    "    quant_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "    finetuned_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=quant_config,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "    finetuned_model = prepare_model_for_kbit_training(finetuned_model)\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=32,\n",
    "        lora_alpha=64,\n",
    "        target_modules=[\"q\", \"v\"],\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=\"SEQ_2_SEQ_LM\"\n",
    "    )\n",
    "\n",
    "    finetuned_model = get_peft_model(finetuned_model, lora_config)\n",
    "    print(\"Base model loaded and prepared for QLORA fine-tuning.\")\n",
    "    clear_memory()\n",
    "\n",
    "print(\"Finetuned model is on device:\", next(finetuned_model.parameters()).device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, BitsAndBytesConfig\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, get_peft_model_state_dict\n",
    "import torch\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "model_name = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "to_train = True\n",
    "try:\n",
    "    finetuned_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "    finetuned_model = finetuned_model.to(device)\n",
    "    print(\"Fine-tuned model loaded successfully.\")\n",
    "    to_train = False\n",
    "except Exception as e:\n",
    "    print(\"Fine-tuned model not found. Initializing model for QLORA fine-tuning...\")\n",
    "\n",
    "    quant_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "    finetuned_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=quant_config,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "    # Disable use_cache for compatibility with gradient checkpointing\n",
    "    finetuned_model.config.use_cache = False\n",
    "\n",
    "    # Enable gradient checkpointing with explicit use_reentrant=False\n",
    "    finetuned_model.gradient_checkpointing_enable(use_reentrant=False)\n",
    "\n",
    "    finetuned_model = prepare_model_for_kbit_training(finetuned_model)\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=32,\n",
    "        lora_alpha=64,\n",
    "        target_modules=[\"q\", \"v\"],\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=\"SEQ_2_SEQ_LM\"\n",
    "    )\n",
    "\n",
    "    finetuned_model = get_peft_model(finetuned_model, lora_config)\n",
    "    print(\"Base model loaded and prepared for QLORA fine-tuning.\")\n",
    "    clear_memory()\n",
    "\n",
    "# TRAINING CONFIG\n",
    "num_train_epochs = 5  \n",
    "per_device_train_batch_size = 16\n",
    "effective_batch_size = 64\n",
    "accumulation_steps = effective_batch_size // per_device_train_batch_size\n",
    "learning_rate = 2e-4\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    batch_size=per_device_train_batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "finetuned_model.train()\n",
    "optimizer = torch.optim.AdamW(finetuned_model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_train_epochs}\")\n",
    "    epoch_loss = 0.0\n",
    "    optimizer.zero_grad()\n",
    "    for i, batch in enumerate(tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\", leave=False)):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = finetuned_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss / accumulation_steps\n",
    "        loss.backward()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1} average loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Evaluation Metrics:\n",
      "====================================================================================================\n",
      "ROUGE: {'rouge1': 0.8177750163270219, 'rouge2': 0.6984209538302064, 'rougeL': 0.7908273408377583, 'rougeLsum': 0.7909292764194}\n",
      "BLEU: {'bleu': 0.5528253924023702, 'precisions': [0.8523966800184269, 0.7132143843741368, 0.6239086066580287, 0.5472419708545557], 'brevity_penalty': 0.8190255090486739, 'length_ratio': 0.8335833742836408, 'translation_length': 256146, 'reference_length': 307283}\n",
      "Fuzzy Match Score: 89.36%\n",
      "Exact Match Accuracy: 26.63%\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# --- EVALUATION ON TEST SET ---\n",
    "\n",
    "print(\"Evaluating on test set...\")\n",
    "all_human_responses = []\n",
    "all_model_responses = []\n",
    "\n",
    "# Create a DataLoader for the test set; here batch size can be higher.\n",
    "test_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"test\"],\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "finetuned_model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_dataloader, desc=\"Evaluating\", leave=False):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        if input_ids.dim() == 1:\n",
    "            input_ids = input_ids.unsqueeze(0)\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        if attention_mask.dim() == 1:\n",
    "            attention_mask = attention_mask.unsqueeze(0)\n",
    "        # Generate responses\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        generated_ids = finetuned_model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=100,\n",
    "            num_beams=5,\n",
    "            repetition_penalty=1.2,\n",
    "            temperature=0.1,\n",
    "            early_stopping=True,\n",
    "        )\n",
    "        # Decode outputs\n",
    "        outputs_decoded = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        all_model_responses.extend(outputs_decoded)\n",
    "        # Also keep the human responses from the original dataset (not tokenized)\n",
    "        all_human_responses.extend(batch[\"labels\"])  # Note: you may wish to decode labels separately\n",
    "\n",
    "# If needed, you can convert the gold labels back from tokens using your dataset\n",
    "# For this example, we assume your original 'test' split in dataset has the human responses.\n",
    "all_human_responses = [ex for ex in dataset[\"test\"][\"response\"]]\n",
    "\n",
    "# Compute evaluation metrics using evaluate library\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "orig_rouge = rouge.compute(\n",
    "    predictions=all_model_responses,\n",
    "    references=all_human_responses,\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "orig_bleu = bleu.compute(\n",
    "    predictions=all_model_responses,\n",
    "    references=[[ref] for ref in all_human_responses]\n",
    ")\n",
    "\n",
    "# Compute a fuzzy match score and exact match accuracy\n",
    "def normalize_sql(sql):\n",
    "    return \" \".join(sql.strip().lower().split())\n",
    "\n",
    "def compute_exact_match(preds, refs):\n",
    "    matches = sum(1 for pred, ref in zip(preds, refs) if normalize_sql(pred) == normalize_sql(ref))\n",
    "    return 100 * matches / len(preds) if preds else 0\n",
    "\n",
    "def compute_fuzzy_match(preds, refs):\n",
    "    scores = [fuzz.token_set_ratio(pred, ref) for pred, ref in zip(preds, refs)]\n",
    "    return sum(scores) / len(scores) if scores else 0\n",
    "\n",
    "fuzzy_score = compute_fuzzy_match(all_model_responses, all_human_responses)\n",
    "exact_match = compute_exact_match(all_model_responses, all_human_responses)\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(\"=\" * 100)\n",
    "print(\"ROUGE:\", orig_rouge)\n",
    "print(\"BLEU:\", orig_bleu)\n",
    "print(f\"Fuzzy Match Score: {fuzzy_score:.2f}%\")\n",
    "print(f\"Exact Match Accuracy: {exact_match:.2f}%\")\n",
    "print(\"=\" * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q rouge_score nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /teamspace/studios/this_studio/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5f919ad527d41fd91090bca3d197a95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "586700857df844cdb197d4140a4d6c5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc6d999e8d7c4acca1c235da0f304a2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "# Load the ROUGE metric\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "# Load the BLEU metric\n",
    "bleu = evaluate.load(\"bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./finetuned_flan_t5_lora/tokenizer_config.json',\n",
       " './finetuned_flan_t5_lora/special_tokens_map.json',\n",
       " './finetuned_flan_t5_lora/tokenizer.json')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ✅ Save LoRA adapters (not the full model)\n",
    "finetuned_model.save_pretrained(\"./finetuned_flan_t5_lora\", save_adapter=True)\n",
    "tokenizer.save_pretrained(\"./finetuned_flan_t5_lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA adapters and tokenizer saved to ./finetuned_flan_t5_lora\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "save_directory = \"./finetuned_flan_t5_lora\"\n",
    "\n",
    "# Save ONLY the LoRA adapter and config\n",
    "finetuned_model.save_pretrained(save_directory, save_adapter=True)\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "print(f\"LoRA adapters and tokenizer saved to {save_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_index(dataset, idx, tokenizer, model, device=\"cuda\", max_new_tokens=50):\n",
    "    model.eval()\n",
    "\n",
    "    sample = dataset[idx]\n",
    "    input_text = sample[\"input\"] if \"input\" in sample else sample[\"text\"]\n",
    "\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(\n",
    "        input_text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True\n",
    "    ).to(device)\n",
    "\n",
    "    # Generate output\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=max_new_tokens\n",
    "        )\n",
    "\n",
    "    # Decode prediction\n",
    "    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return {\n",
    "        \"input\": input_text,\n",
    "        \"prediction\": prediction,\n",
    "        \"reference\": sample.get(\"output\") or sample.get(\"target\") or None\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QLoRA model and tokenizer have been saved to: ./finetuned_flan_t5_lora_2\n"
     ]
    }
   ],
   "source": [
    "# Define your save directory\n",
    "save_directory = \"./finetuned_flan_t5_lora_2\"\n",
    "\n",
    "# Save only the LoRA adapters and related configuration (ensures adapter_config.json is included)\n",
    "finetuned_model.save_pretrained(save_directory, save_adapter=True)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "print(f\"QLoRA model and tokenizer have been saved to: {save_directory}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Dataset loaded. Number of test samples: 9990\n",
      "--------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      " Context:\n",
      "CREATE TABLE broadband_services (service_id INT, region VARCHAR(255), revenue DECIMAL(10,2)); INSERT INTO broadband_services (service_id, region, revenue) VALUES (1, 'North', 5000), (2, 'South', 7000);\n",
      "\n",
      "Query:\n",
      "What is the total revenue from broadband services for each region?\n",
      "\n",
      "Response:\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE RESPONSE:\n",
      " SELECT region, SUM(revenue) FROM broadband_services GROUP BY region;\n",
      "--------------------------------------------------------------------------------\n",
      "MODEL PREDICTION:\n",
      " SELECT region, SUM(revenue) FROM broadband_services GROUP BY region;\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import gc\n",
    "\n",
    "# Clear memory\n",
    "def clear_memory():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Load the baseline tokenizer\n",
    "# (Assuming that the same base tokenizer was used for fine-tuning)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "# -------------------------------\n",
    "# Inference function definition\n",
    "# -------------------------------\n",
    "\n",
    "def infer_index(sample: dict, model, tokenizer, device, max_new_tokens=200):\n",
    "    \"\"\"\n",
    "    Given a sample from the dataset (with keys 'context', 'query', and optionally 'response'),\n",
    "    this function constructs the prompt, tokenizes it, and generates a prediction using `model`.\n",
    "    \"\"\"\n",
    "    # Create the prompt by concatenating context and query\n",
    "    prompt = f\"Context:\\n{sample['context']}\\n\\nQuery:\\n{sample['query']}\\n\\nResponse:\\n\"\n",
    "    \n",
    "    # Tokenize the prompt (ensure proper truncation/padding if necessary)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "    \n",
    "    # In evaluation mode, generate outputs\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=max_new_tokens,\n",
    "        )\n",
    "    \n",
    "    # Decode the generated ids to get the text output\n",
    "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"prediction\": generated_text,\n",
    "        \"reference\": sample.get(\"response\", None)\n",
    "    }\n",
    "\n",
    "\n",
    "from datasets import load_from_disk\n",
    "dataset = load_from_disk(\"gretelai_dataset\")\n",
    "print(\"Dataset loaded. Number of test samples:\", len(dataset[\"test\"]))\n",
    "\n",
    "sample_idx = 10\n",
    "test_sample = dataset[\"test\"][sample_idx]\n",
    "\n",
    "result = infer_index(test_sample, finetuned_model, tokenizer, device, max_new_tokens=200)\n",
    "\n",
    "# Print the results\n",
    "print(\"-\" * 80)\n",
    "print(\"INPUT PROMPT:\\n\", result[\"prompt\"])\n",
    "print(\"-\" * 80)\n",
    "print(\"REFERENCE RESPONSE:\\n\", result[\"reference\"])\n",
    "print(\"-\" * 80)\n",
    "print(\"MODEL PREDICTION:\\n\", result[\"prediction\"])\n",
    "print(\"-\" * 80)\n",
    "\n",
    "clear_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      " Context:\n",
      "CREATE TABLE container_ships (ship_id INT, ship_name VARCHAR(255), ship_builder VARCHAR(255), year INT, cargo_weight INT);INSERT INTO container_ships (ship_id, ship_name, ship_builder, year, cargo_weight) VALUES (1, 'Ever Given', 'Baosteel', 2010, 210000), (2, 'CMA CGM Marco Polo', 'Daewoo Shipbuilding & Marine Engineering', 2008, 165000);\n",
      "\n",
      "Query:\n",
      "What is the total cargo weight handled by container ships built before 2010, grouped by ship builder?\n",
      "\n",
      "Response:\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE RESPONSE:\n",
      " SELECT ship_builder, SUM(cargo_weight) FROM container_ships WHERE year < 2010 GROUP BY ship_builder;\n",
      "--------------------------------------------------------------------------------\n",
      "MODEL PREDICTION:\n",
      " SELECT ship_builder, SUM(cargo_weight) FROM container_ships WHERE year  2010 GROUP BY ship_builder;\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "sample_idx = 22\n",
    "test_sample = dataset[\"test\"][sample_idx]\n",
    "\n",
    "result = infer_index(test_sample, finetuned_model, tokenizer, device, max_new_tokens=200)\n",
    "\n",
    "# Print the results\n",
    "print(\"-\" * 80)\n",
    "print(\"INPUT PROMPT:\\n\", result[\"prompt\"])\n",
    "print(\"-\" * 80)\n",
    "print(\"REFERENCE RESPONSE:\\n\", result[\"reference\"])\n",
    "print(\"-\" * 80)\n",
    "print(\"MODEL PREDICTION:\\n\", result[\"prediction\"])\n",
    "print(\"-\" * 80)\n",
    "\n",
    "clear_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      " Context:\n",
      "CREATE TABLE region_rainfall (region TEXT, date DATE, rainfall INTEGER);\n",
      "\n",
      "Query:\n",
      "What is the maximum rainfall recorded for each region in the past year?\n",
      "\n",
      "Response:\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE RESPONSE:\n",
      " SELECT region, MAX(rainfall) as max_rainfall FROM region_rainfall WHERE date >= DATEADD(year, -1, GETDATE()) GROUP BY region;\n",
      "--------------------------------------------------------------------------------\n",
      "MODEL PREDICTION:\n",
      " SELECT region, MAX(rainfall) FROM region_rainfall WHERE date >= DATEADD(year, -1, GETDATE()) GROUP BY region;\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Choose an index from the test set (e.g., first sample)\n",
    "sample_idx = 32\n",
    "test_sample = dataset[\"test\"][sample_idx]\n",
    "\n",
    "# Generate inference on the sample using the in-memory model (finetuned_model)\n",
    "result = infer_index(test_sample, finetuned_model, tokenizer, device, max_new_tokens=200)\n",
    "\n",
    "# Print the results\n",
    "print(\"-\" * 80)\n",
    "print(\"INPUT PROMPT:\\n\", result[\"prompt\"])\n",
    "print(\"-\" * 80)\n",
    "print(\"REFERENCE RESPONSE:\\n\", result[\"reference\"])\n",
    "print(\"-\" * 80)\n",
    "print(\"MODEL PREDICTION:\\n\", result[\"prediction\"])\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Cleanup memory if needed\n",
    "clear_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      " Context:\n",
      "CREATE TABLE fish_biomass (species TEXT, population REAL, biomass REAL); INSERT INTO fish_biomass (species, population, biomass) VALUES ('Cod', 10000, 200000), ('Herring', 20000, 300000);\n",
      "\n",
      "Query:\n",
      "What is the total biomass of fish in the Barents Sea?\n",
      "\n",
      "Response:\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE RESPONSE:\n",
      " SELECT SUM(biomass) FROM fish_biomass WHERE species IN ('Cod', 'Herring', 'Capelin');\n",
      "--------------------------------------------------------------------------------\n",
      "MODEL PREDICTION:\n",
      " SELECT SUM(biomass) FROM fish_biomass WHERE species IN ('Cod', 'Herring');\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Choose an index from the test set (e.g., first sample)\n",
    "sample_idx = 41\n",
    "test_sample = dataset[\"test\"][sample_idx]\n",
    "\n",
    "# Generate inference on the sample using the in-memory model (finetuned_model)\n",
    "result = infer_index(test_sample, finetuned_model, tokenizer, device, max_new_tokens=200)\n",
    "\n",
    "# Print the results\n",
    "print(\"-\" * 80)\n",
    "print(\"INPUT PROMPT:\\n\", result[\"prompt\"])\n",
    "print(\"-\" * 80)\n",
    "print(\"REFERENCE RESPONSE:\\n\", result[\"reference\"])\n",
    "print(\"-\" * 80)\n",
    "print(\"MODEL PREDICTION:\\n\", result[\"prediction\"])\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Cleanup memory if needed\n",
    "clear_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      " Context:\n",
      "CREATE TABLE SpaceLaunchs (LaunchID INT, Country VARCHAR(50), SatelliteID INT); INSERT INTO SpaceLaunchs (LaunchID, Country, SatelliteID) VALUES (1, 'USA', 101), (2, 'Russia', 201), (3, 'China', 301), (4, 'India', 401), (5, 'Japan', 501);\n",
      "\n",
      "Query:\n",
      "What is the total number of satellites launched by country in the SpaceLaunchs table?\n",
      "\n",
      "Response:\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE RESPONSE:\n",
      " SELECT Country, COUNT(SatelliteID) AS TotalSatellites FROM SpaceLaunchs GROUP BY Country;\n",
      "--------------------------------------------------------------------------------\n",
      "MODEL PREDICTION:\n",
      " SELECT Country, COUNT(*) as TotalSatellites FROM SpaceLaunches GROUP BY Country;\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Choose an index from the test set (e.g., first sample)\n",
    "sample_idx = 120\n",
    "test_sample = dataset[\"test\"][sample_idx]\n",
    "\n",
    "# Generate inference on the sample using the in-memory model (finetuned_model)\n",
    "result = infer_index(test_sample, finetuned_model, tokenizer, device, max_new_tokens=200)\n",
    "\n",
    "# Print the results\n",
    "print(\"-\" * 80)\n",
    "print(\"INPUT PROMPT:\\n\", result[\"prompt\"])\n",
    "print(\"-\" * 80)\n",
    "print(\"REFERENCE RESPONSE:\\n\", result[\"reference\"])\n",
    "print(\"-\" * 80)\n",
    "print(\"MODEL PREDICTION:\\n\", result[\"prediction\"])\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Cleanup memory if needed\n",
    "clear_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      " Context:\n",
      "CREATE TABLE claim (claim_id INT, processed_by VARCHAR(50)); INSERT INTO claim VALUES (1, 'Laura Smith'); INSERT INTO claim VALUES (2, 'Maria Silva');\n",
      "\n",
      "Query:\n",
      "Which claims were processed by the claims adjuster 'Maria Silva'?\n",
      "\n",
      "Response:\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE RESPONSE:\n",
      " SELECT claim_id FROM claim WHERE processed_by = 'Maria Silva';\n",
      "--------------------------------------------------------------------------------\n",
      "MODEL PREDICTION:\n",
      " SELECT claim_id FROM claim WHERE processed_by = 'Maria Silva';\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Choose an index from the test set (e.g., first sample)\n",
    "sample_idx = 190\n",
    "test_sample = dataset[\"test\"][sample_idx]\n",
    "\n",
    "# Generate inference on the sample using the in-memory model (finetuned_model)\n",
    "result = infer_index(test_sample, finetuned_model, tokenizer, device, max_new_tokens=200)\n",
    "\n",
    "# Print the results\n",
    "print(\"-\" * 80)\n",
    "print(\"INPUT PROMPT:\\n\", result[\"prompt\"])\n",
    "print(\"-\" * 80)\n",
    "print(\"REFERENCE RESPONSE:\\n\", result[\"reference\"])\n",
    "print(\"-\" * 80)\n",
    "print(\"MODEL PREDICTION:\\n\", result[\"prediction\"])\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Cleanup memory if needed\n",
    "clear_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      " Context:\n",
      "CREATE TABLE environmental_impact (site_name VARCHAR(50), co2_emissions INT, waste_generation INT); INSERT INTO environmental_impact (site_name, co2_emissions, waste_generation) VALUES ('Site Alpha', 1200, 500), ('Site Bravo', 1800, 800), ('Site Charlie', 2500, 1000);\n",
      "\n",
      "Query:\n",
      "What are the total CO2 emissions for all mining sites in the 'environmental_impact' table?\n",
      "\n",
      "Response:\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE RESPONSE:\n",
      " SELECT SUM(co2_emissions) FROM environmental_impact;\n",
      "--------------------------------------------------------------------------------\n",
      "MODEL PREDICTION:\n",
      " SELECT SUM(co2_emissions) FROM environmental_impact;\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Choose an index from the test set (e.g., first sample)\n",
    "sample_idx = 2010\n",
    "test_sample = dataset[\"test\"][sample_idx]\n",
    "\n",
    "# Generate inference on the sample using the in-memory model (finetuned_model)\n",
    "result = infer_index(test_sample, finetuned_model, tokenizer, device, max_new_tokens=200)\n",
    "\n",
    "# Print the results\n",
    "print(\"-\" * 80)\n",
    "print(\"INPUT PROMPT:\\n\", result[\"prompt\"])\n",
    "print(\"-\" * 80)\n",
    "print(\"REFERENCE RESPONSE:\\n\", result[\"reference\"])\n",
    "print(\"-\" * 80)\n",
    "print(\"MODEL PREDICTION:\\n\", result[\"prediction\"])\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Cleanup memory if needed\n",
    "clear_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      " Context:\n",
      "CREATE TABLE clients (client_id INT, name VARCHAR(50)); CREATE TABLE cases (case_id INT, client_id INT, billing_amount DECIMAL(10,2)); INSERT INTO clients (client_id, name) VALUES (1, 'Smith'), (2, 'Johnson'), (3, 'Williams'), (4, 'Brown'); INSERT INTO cases (case_id, client_id, billing_amount) VALUES (1, 1, 3000.00), (2, 2, 6000.00), (3, 3, 7000.00), (4, 4, 4000.00);\n",
      "\n",
      "Query:\n",
      "List all clients who have paid exactly $4000 in total billing amount?\n",
      "\n",
      "Response:\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE RESPONSE:\n",
      " SELECT clients.name FROM clients INNER JOIN cases ON clients.client_id = cases.client_id GROUP BY clients.name HAVING SUM(billing_amount) = 4000;\n",
      "--------------------------------------------------------------------------------\n",
      "MODEL PREDICTION:\n",
      " SELECT clients.name FROM clients INNER JOIN cases ON clients.client_id = cases.client_id WHERE cases.billing_amount = 4000;\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Choose an index from the test set (e.g., first sample)\n",
    "sample_idx = 2100\n",
    "test_sample = dataset[\"test\"][sample_idx]\n",
    "\n",
    "# Generate inference on the sample using the in-memory model (finetuned_model)\n",
    "result = infer_index(test_sample, finetuned_model, tokenizer, device, max_new_tokens=200)\n",
    "\n",
    "# Print the results\n",
    "print(\"-\" * 80)\n",
    "print(\"INPUT PROMPT:\\n\", result[\"prompt\"])\n",
    "print(\"-\" * 80)\n",
    "print(\"REFERENCE RESPONSE:\\n\", result[\"reference\"])\n",
    "print(\"-\" * 80)\n",
    "print(\"MODEL PREDICTION:\\n\", result[\"prediction\"])\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Cleanup memory if needed\n",
    "clear_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      " Context:\n",
      "CREATE TABLE Water_Usage (Year INT, Sector VARCHAR(20), Volume INT); INSERT INTO Water_Usage (Year, Sector, Volume) VALUES (2019, 'Industry', 12300000), (2018, 'Industry', 12000000), (2020, 'Industry', 12500000);\n",
      "\n",
      "Query:\n",
      "What is the total volume of water consumed by the industrial sector in the state of Florida in 2020?\n",
      "\n",
      "Response:\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE RESPONSE:\n",
      " SELECT SUM(Volume) FROM Water_Usage WHERE Year = 2020 AND Sector = 'Industry';\n",
      "--------------------------------------------------------------------------------\n",
      "MODEL PREDICTION:\n",
      " SELECT SUM(Volume) FROM Water_Usage WHERE Sector = 'Industry' AND Year = 2020;\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Choose an index from the test set (e.g., first sample)\n",
    "sample_idx = 1010\n",
    "test_sample = dataset[\"test\"][sample_idx]\n",
    "\n",
    "# Generate inference on the sample using the in-memory model (finetuned_model)\n",
    "result = infer_index(test_sample, finetuned_model, tokenizer, device, max_new_tokens=200)\n",
    "\n",
    "# Print the results\n",
    "print(\"-\" * 80)\n",
    "print(\"INPUT PROMPT:\\n\", result[\"prompt\"])\n",
    "print(\"-\" * 80)\n",
    "print(\"REFERENCE RESPONSE:\\n\", result[\"reference\"])\n",
    "print(\"-\" * 80)\n",
    "print(\"MODEL PREDICTION:\\n\", result[\"prediction\"])\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Cleanup memory if needed\n",
    "clear_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      " Context:\n",
      "CREATE TABLE Dispensaries (id INT, name VARCHAR(255), city VARCHAR(255), state VARCHAR(255));CREATE TABLE Inventory (id INT, dispensary_id INT, weight DECIMAL(10, 2), product_type VARCHAR(255), month INT, year INT);INSERT INTO Dispensaries (id, name, city, state) VALUES (1, 'Green Leaf', 'Denver', 'CO');INSERT INTO Inventory (id, dispensary_id, weight, product_type, month, year) VALUES (1, 1, 250, 'flower', 4, 2021);\n",
      "\n",
      "Query:\n",
      "What was the total weight of cannabis flower sold by each dispensary in the city of Denver in the month of April 2021?\n",
      "\n",
      "Response:\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE RESPONSE:\n",
      " SELECT d.name, SUM(i.weight) as total_weight FROM Dispensaries d JOIN Inventory i ON d.id = i.dispensary_id WHERE d.city = 'Denver' AND i.product_type = 'flower' AND i.month = 4 AND i.year = 2021 GROUP BY d.name;\n",
      "--------------------------------------------------------------------------------\n",
      "MODEL PREDICTION:\n",
      " SELECT d.name, SUM(i.weight) as total_weight FROM Dispensaries d INNER JOIN Inventory i ON d.id = i.dispensary_id WHERE i.product_type = 'flower' AND i.month = 4 AND i.year = 2021 GROUP BY d.name;\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Choose an index from the test set (e.g., first sample)\n",
    "sample_idx = 1011\n",
    "test_sample = dataset[\"test\"][sample_idx]\n",
    "\n",
    "# Generate inference on the sample using the in-memory model (finetuned_model)\n",
    "result = infer_index(test_sample, finetuned_model, tokenizer, device, max_new_tokens=200)\n",
    "\n",
    "# Print the results\n",
    "print(\"-\" * 80)\n",
    "print(\"INPUT PROMPT:\\n\", result[\"prompt\"])\n",
    "print(\"-\" * 80)\n",
    "print(\"REFERENCE RESPONSE:\\n\", result[\"reference\"])\n",
    "print(\"-\" * 80)\n",
    "print(\"MODEL PREDICTION:\\n\", result[\"prediction\"])\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Cleanup memory if needed\n",
    "clear_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      " Context:\n",
      "CREATE TABLE Departments (DepartmentID INT PRIMARY KEY, DepartmentName VARCHAR(50), BudgetForDisabilityAccommodations DECIMAL(10,2), NumberOfStudentsWithDisabilities INT); CREATE TABLE Universities (UniversityID INT PRIMARY KEY, UniversityName VARCHAR(50), UniversityLocation VARCHAR(50)); CREATE TABLE UniversityDepartments (UniversityDepartmentID INT PRIMARY KEY, UniversityID INT, DepartmentID INT, FOREIGN KEY (UniversityID) REFERENCES Universities(UniversityID), FOREIGN KEY (DepartmentID) REFERENCES Departments(DepartmentID));\n",
      "\n",
      "Query:\n",
      "What is the total budget for disability accommodations in departments with more than 20% of students with disabilities in a university in Canada?\n",
      "\n",
      "Response:\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "REFERENCE RESPONSE:\n",
      " SELECT SUM(BudgetForDisabilityAccommodations) as TotalBudget FROM UniversityDepartments ud JOIN Departments d ON ud.DepartmentID = d.DepartmentID JOIN Universities u ON ud.UniversityID = u.UniversityID WHERE u.UniversityLocation LIKE '%Canada%' GROUP BY ud.UniversityID HAVING AVG(d.NumberOfStudentsWithDisabilities) > 0.2*AVG(d.TotalStudents);\n",
      "--------------------------------------------------------------------------------\n",
      "MODEL PREDICTION:\n",
      " SELECT SUM(BudgetForDisabilityAccommodations) FROM Departments JOIN UniversityDepartments ON Departments.DepartmentID = UniversityDepartments.DepartmentID WHERE NumberOfStudentsWithDisabilities > 20 AND UniversityLocation = 'Canada';\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Choose an index from the test set (e.g., first sample)\n",
    "sample_idx = 1012\n",
    "test_sample = dataset[\"test\"][sample_idx]\n",
    "\n",
    "# Generate inference on the sample using the in-memory model (finetuned_model)\n",
    "result = infer_index(test_sample, finetuned_model, tokenizer, device, max_new_tokens=200)\n",
    "\n",
    "# Print the results\n",
    "print(\"-\" * 80)\n",
    "print(\"INPUT PROMPT:\\n\", result[\"prompt\"])\n",
    "print(\"-\" * 80)\n",
    "print(\"REFERENCE RESPONSE:\\n\", result[\"reference\"])\n",
    "print(\"-\" * 80)\n",
    "print(\"MODEL PREDICTION:\\n\", result[\"prediction\"])\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Cleanup memory if needed\n",
    "clear_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
